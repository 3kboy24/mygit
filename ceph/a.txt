1.dmesg 错误log 关于117
[20217.456194] XFS (sda1): Metadata corruption detected at xfs_dir3_data_read_verify+0x48/0xc4, xfs_dir3_data block 0x368b310d0[20217.467456] XFS (sda1): Unmount and run xfs_repair[20217.472257] XFS (sda1): First 64 bytes of corrupted metadata buffer:[20217.478635] eb541000: 58 46 53 42 00 00 10 00 00 00 00 00 74 5c 24 51  XFSB........t\$Q[20217.486662] eb541010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................[20217.494686] eb541020: 36 7e 81 73 da ad 40 54 a1 cc 06 80 55 1f e1 f4  6~.s..@T....U...[20217.502706] eb541030: 00 00 00 00 40 00 00 07 ff ff ff ff ff ff ff ff  ....@...........[20217.511084] XFS (sda1): metadata I/O error: block 0x368b310d0 ("xfs_trans_read_buf_map") error 117 numblks 8

2.Linux 源代码关于错误117#define EUCLEAN         117     /* Structure needs cleaning */

3. osd log
-3> 2017-04-24 16:22:19.141789 addd1b10  0 filestore(/var/lib/ceph/osd/ceph-6)  error (39) Directory not empty not handled on operation 0x8a3b1348 (147706.0.1, or op 1, counting from 0)    -2> 2017-04-24 16:22:19.141828 addd1b10  0 filestore(/var/lib/ceph/osd/ceph-6) ENOTEMPTY suggests garbage data in osd data dir    -1> 2017-04-24 16:22:19.141833 addd1b10  0 filestore(/var/lib/ceph/osd/ceph-6)  transaction dump:{    "ops": [        {            "op_num": 0,            "op_name": "remove",            "collection": "3.4c_head",            "oid": "#3:32000000::::head#"        },        {            "op_num": 1,            "op_name": "rmcoll",            "collection": "3.4c_head"        }    ]}     0> 2017-04-24 16:22:19.144410 addd1b10 -1 os/filestore/FileStore.cc: In function 'void FileStore::_do_transaction(ObjectStore::Transaction&, uint64_t, int, ThreadPool::TPHandle*)' thread addd1b10 time 2017-04-24 16:22:19.141924os/filestore/FileStore.cc: 2912: FAILED assert(0 == "unexpected error")4.直接用ll命令验证117错误（Structure needs cleaning）
mnvadmin@mnc04:/var/lib/ceph/osd/ceph-6/current$ ll 3.4c_headls: reading directory 3.4c_head: Structure needs cleaning

sudo /libexec/ceph/ceph-osd-prestart.sh --cluster=ceph --id=

解决办法：
解决办法很简单，将ceph集群需要使用的所有磁盘权限，所属用户、用户组改给ceph
chown ceph:ceph /dev/sdd1
问题延伸：
此问题本次修复后，系统重启磁盘权限会被修改回，导致osd服务无法正常启动，这个权限问题很坑，写了个for 循环，加入到rc.local,每次系统启动自动修改磁盘权限；
for i in a b c d e f g h i l j k;do chown ceph.ceph /dev/sd"$i"*;done
查找ceph资料，发现这其实是一个bug，社区暂未解决。
参考信息：
http://tracker.ceph.com/issues/13833

remote.py
sshpass -p ?mnvadmin? ssh mnvadmin@112.90.18.177 -p 11022


ceph-deploy disk zap mnc08:sda
ceph-deploy osd prepare mnc08:/dev/sda
ssh mnc08
chown ceph:ceph /dev/sda2
exit
ceph-deploy osd activate mnc08:/dev/sda1:/dev/sda2 

ceph-deploy disk zap mnc11:sda mnc11:sdb
ceph-deploy osd prepare mnc11:sda:/dev/sdb
ssh mnc11
chown ceph:ceph /dev/sdb1
exit
ceph-deploy osd activate mnc11:/dev/sda1:/dev/sdb1

OSD创建脚本
#$1 = hostname, $2 = diskname, $3 = osd number
parttion_name = sda
hostname = mnc11

ceph-deploy disk zap $1:$2
ceph-deploy osd prepare $1:/dev/$2
ssh $1 "chown ceph:ceph /dev/{$2}2"
ceph-deploy osd activate $1:/dev/{$2}1:/dev/{$2}2 
ssh $1 "sudo /libexec/ceph/ceph-osd-prestart.sh --cluster=ceph --id=$3"

OSD删除脚本
# $1=osd number
#!/bin/sh
ceph osd down osd.$1
ceph osd out osd.$1
ceph osd crush remove osd.$1
ceph osd rm $1
ceph auth del osd.$1

带空格的scp
scp pana@192.168.3.33:"~/zhaohedong/remote\ cluster"

带端口的scp
scp -P 11022 -r mnvadmin@112.90.18.177:~/ceph-cluster .

删除pool命令
ceph osd pool delete data data --yes-i-really-really-mean-it

ntp时间同步
sudo service ntp stop
sudo ntpdate time.nist.gov

pool删除
ceph osd pool delete {poolname} {poolname} --yes-i-really-really-mean-it

删除cephfs
ceph stop mds
ceph mds rm 0
ceph fs ls
ceph fs rm {fs-name} --yes-i-really-mean-it
ceph fs rm arm_fs --yes-i-really-mean-it
ceph osd pool delete data data --yes-i-really-really-mean-it
ceph osd pool delete metadata metadata --yes-i-really-really-mean-it

添加ceph更新源
wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -
echo deb https://download.ceph.com/debian-jewel/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
sudo apt-get update && sudo apt-get install ceph-deploy

创建mds
ceph-deploy mds create {hostname}

创建存储池
ceph osd pool create cephfs_data <pg_num>
ceph osd pool create cephfs_metadata <pg_num>
ceph fs new <fs_name> <metadata> <data>

ceph osd pool create data 128
ceph osd pool create metadata 128
ceph fs new arm_fs metadata data

挂载cephfs
sudo mount -t ceph 192.168.99.22:6789:/ /mnt/mycephfs -o name=admin,secretfile=admin.secret

pg数量调整
1.调整pg
  ceph osd pool set <poolname> pg_num <new_pgnum>
2.等待集群状态恢复到正常
3.调整pgp
  ceph osd pool set <poolname> pgp_num <new_pgnum>

fio测试
fio -filename=/dev/sda1 -direct=1 -iodepth 1 -thread -rw=write -ioengine=libaio -bs=4m -size=10G -numjobs=10 -runtime=100 -group_reporting -name=writetest
fio -filename=/mnt/mycephfs/test -direct=1 -iodepth 1 -thread -rw=randread -ioengine=libaio -bs=4k -size=10G -numjobs=10 -runtime=100 -group_reporting -name=randreadtest
fio -filename=/mnt/mycephfs/test -direct=1 -iodepth 1 -thread -rw=write -ioengine=libaio -bs=4m -size=10G -numjobs=10 -runtime=100 -group_reporting -name=writetest
fio -filename=/mnt/mycephfs/test -direct=1 -iodepth 1 -thread -rw=write -ioengine=libaio -bs=4m -size=10G -numjobs=48 -runtime=100 -group_reporting -name=writetest

dd测试
  首先要了解两个特殊的设备：
  /dev/null：回收站、无底洞
  /dev/zero：产生字符
   
  Ø 测试磁盘写能力
  time dd if=/dev/zero of=./test.dbf bs=1M count=1000
  因为/dev//zero是一个伪设备，它只产生空字符流，对它不会产生IO，所以，IO都会集中在of文件中，of文件只用于写，所以这个命令相当于测试磁盘的写能力。
   
  Ø 测试磁盘读能力
  time dd if=/dev/sdb1 of=/dev/null bs=1M
  因为/dev/sdb1是一个物理分区，对它的读取会产生IO，/dev/null是伪设备，相当于黑洞，of到该设备不会产生IO，所以，这个命令的IO只发生在/dev/sdb1上，也相当于测试磁盘的读能力。
   
  Ø 测试同时读写能力
  time dd if=/dev/sdb1 of=/test1.dbf bs=1M
  这个命令下，一个是物理分区，一个是实际的文件，对它们的读写都会产生IO（对/dev/sdb1是读，对/test1.dbf是写），假设他们都在一个磁盘中，这个命令就相当于测试磁盘的同时读写能力

开机修改硬盘分区权限
sudo vi /etc/profile
for i in a b;do sudo chown ceph.ceph /dev/sd"$i"*;done

ssh修改硬盘权限
for i in a b;do sudo chown ceph.ceph /dev/sd*;done
ssh mnc01 "sudo echo 'for i in a b;do sudo chown ceph.ceph /dev/sd'$i'*;done' | sudo tee -a /etc/profile"

ceph查看配置
ceph daemon {daemon-type}.{id} config show | less
sudo ceph daemon osd.2 config show | less

ceph修改配置
ceph tell {daemon-type}.{id or *} injectargs --{name} {value} [--{name} {value}]
ceph tell osd.0 injectargs --debug-osd 20 --debug-ms 1

硬盘卸载
sudo start ceph-osd-all
sudo stop ceph-osd-all
sudo umount /dev/sda1
sudo umount /dev/sdb1

parted分区
parted /dev/sda
mklabel gpt
mkpart
分区名称？  []?     		//设置分区名
文件系统类型？  [ext2]?      	//直接回车
起始点？ 0   			//分区开始
结束点？ -1   			//分区结束 -1为全部硬盘空间
(parted) p   			//查看分区结果
mkfs.xfs -f /dev/sda1		//格式化分区

硬盘格式化
[mnc01][WARNIN] create_partition: Creating data partition num 1 size 0 on /dev/sda
[mnc01][WARNIN] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:b8eb5cd5-7384-49aa-bc4c-604904d54ef6 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sda
[mnc01][DEBUG ] The operation has completed successfully.
[mnc01][WARNIN] update_partition: Calling partprobe on created device /dev/sda
[mnc01][WARNIN] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[mnc01][WARNIN] command: Running command: /usr/bin/flock -s /dev/sda /sbin/partprobe /dev/sda
[mnc01][WARNIN] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[mnc01][WARNIN] get_dm_uuid: get_dm_uuid /dev/sda uuid path is /sys/dev/block/8:0/dm/uuid
[mnc01][WARNIN] get_dm_uuid: get_dm_uuid /dev/sda uuid path is /sys/dev/block/8:0/dm/uuid
[mnc01][WARNIN] get_dm_uuid: get_dm_uuid /dev/sda1 uuid path is /sys/dev/block/8:1/dm/uuid
[mnc01][WARNIN] populate_data_path_device: Creating xfs fs on /dev/sda1
[mnc01][WARNIN] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sda1

deb安装包权限
sudo chmod ug+rw,o+r *

deb安装包路径
mkdir ~/packs
/home/pana/zhaohedong/packs/

sources.list修改
sudo vi /etc/apt/sources.list
deb file:/home/pana/zhaohedong packs/

软件包信息
cd ~
dpkg-scanpackages packs /dev/null |gzip > packs/Packages.gz -r

sudo apt-get update
sudo apt-get install ceph

scp ceph_detect_init-1.0.1-py2.7.egg mnc41:/lib/python2.7/site-packages/
scp ceph_detect_init-1.0.1-py2.7.egg mnc41:/usr/lib/python2.7/dist-packages/

pip安装

get-pip.py获取
wget https://bootstrap.pypa.io/get-pip.py 

官网参考地址
https://pip.pypa.io/en/stable/installing/#id8

pip安装
python get-pip.py

pip会安装到下面路径
/usr/local/lib/python2.7/dist-packages

到这步，还不能pip命令还需要使用apt-get命令安装python-pip
The program 'pip' is currently not installed. You can install it by typing:
sudo apt install python-pip

关于python egg的安装
安装pip之后，已经默认安装了setuptools
easy_install xxxx.egg

取得每个osd上pg的数量
ceph pg dump | awk '  
 /^pg_stat/ { col=1; while($col!="up") {col++}; col++ }  
 /^[0-9a-f]+\.[0-9a-f]+/ { match($0,/^[0-9a-f]+/); pool=substr($0, RSTART, RLENGTH); poollist[pool]=0;  
 up=$col; i=0; RSTART=0; RLENGTH=0; delete osds; while(match(up,/[0-9]+/)>0) { osds[++i]=substr(up,RSTART,RLENGTH); up = substr(up, RSTART+RLENGTH) }  
 for(i in osds) {array[osds[i],pool]++; osdlist[osds[i]];}  
}  
END {  
 printf("\n");  
 printf("pool :\t"); for (i in poollist) printf("%s\t",i); printf("| SUM \n");  
 for (i in poollist) printf("--------"); printf("----------------\n");  
 for (i in osdlist) { printf("osd.%i\t", i); sum=0;  
 for (j in poollist) { printf("%i\t", array[i,j]); sum+=array[i,j]; poollist[j]+=array[i,j] }; printf("| %i\n",sum) }  
 for (i in poollist) printf("--------"); printf("----------------\n");  
 printf("SUM :\t"); for (i in poollist) printf("%s\t",poollist[i]); printf("|\n");  
}' 

获取crush map
ceph osd getcrushmap -o mycrushmap

反编译crush map
crushtool -d mycrushmap -o mycrushmap_decompiled

编辑crushmap
vi mycrushmap_decompiled

编译crush map
crushtool -c mycrushmap_decompiled -o mycrushmap_new

设定crush map
ceph osd setcrushmap -i mycrushmap_new 

获取pg map
ceph pg map {pg-id}

获取pg统计
ceph pg {pg-id} query

获取所有pg
ceph pg dump 

环境变更
sudo cp 50-rbd.rules 60-ceph-by-parttypeuuid.rules 95-ceph-osd.rules /lib/udev/rules.d/
sudo cp /lib/python2.7/site-packages/* /usr/local/lib/python2.7/site-packages/
wget https://bootstrap.pypa.io/get-pip.py
./configure --enable-unicode=UCS4
打开./Modules/Setup文件下的zlib，需要编译zlib
undefined symbol: _PyUnicodeUCS4_AsDefaultEncodedString

Traceback (most recent call last):
  File "get-pip.py", line 20061, in <module>
    main()
  File "get-pip.py", line 188, in main
    fp.write(b85decode(DATA.replace(b"\n", b"")))
IOError: [Errno 28] No space left on device

umount /tmp
python get-pip.py
sudo -i 
easy_install ceph_detect_init-1.0.1-py2.7.egg 

vi /etc/enviroment
PYTHONHOME=/usr/local
PYTHONPATH=/usr/local/lib/python2.7

ln -s /bin/ceph-rbdnamer /usr/bin/ceph-rbdnamer
ln -s /usr/local/bin/ceph-disk /usr/bin/ceph-disk

ceph-deploy --cluster xfscluster new mnc75
ceph-deploy --cluster xfscluster mon create-initial
ceph-deploy --cluster xfscluster disk zap mnc75:sda
ceph-deploy --cluster xfscluster osd prepare mnc75:sda
ceph-deploy --cluster xfscluster osd activate mnc75:/dev/sda1

ceph-deploy --cluster xfscluster disk zap mnc76:sda
ceph-deploy --cluster xfscluster osd prepare mnc76:sda
ceph-deploy --cluster xfscluster osd activate mnc76:/dev/sda1

ceph-deploy --cluster xfscluster2 new mnc76
ceph-deploy --cluster xfscluster2 mon create-initial
ceph-deploy --cluster xfscluster2 disk zap mnc76:sda
ceph-deploy --cluster xfscluster2 osd prepare mnc76:sda
ceph-deploy --cluster xfscluster2 osd activate mnc76:/dev/sda1

ceph-deploy --cluster xfsclustera new mnc51
ceph-deploy --cluster xfsclustera mon create-initial
ceph-deploy --cluster xfsclustera disk zap mnc51:sda
ceph-deploy --cluster xfsclustera osd prepare mnc51:sda
ceph-deploy --cluster xfsclustera osd activate mnc51:/dev/sda1
ceph-deploy --cluster xfsclustera disk zap mnc51:sdb
ceph-deploy --cluster xfsclustera osd prepare mnc51:sdb
ceph-deploy --cluster xfsclustera osd activate mnc51:/dev/sdb1

心得：
问题1：unable to find /etc/ceph/ceph.client.admin.keyring 
解决：进行mon create-initial时候，要注意fsid是否同ceph.conf文件中的fsid相同，如果不同会导致之后无法获得ceph.client.admin.keyring.
解决办法是删除mon initial节点下的/var/lib/mon/下的monitor文件夹

问题2: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-i
解决: 创建mon节点后，要在mon节点上执行ceph auth list来查看，是否有四个key client.admin, client.bootstrap-osd, client.boostrap-mds, client.bootstrap-rgw
如果没有client.bootstrap-osd则会导致activate osd失败

问题3: osd节点重启后，无法正常挂载硬盘
解决: 重点关注/var/log/upstart/下的相关log

cd ~/ceph-cluster
./del_osd.sh 46
./del_osd.sh 47

cd ~/zhao/udev
sudo cp 50-rbd.rules 60-ceph-by-parttypeuuid.rules 95-ceph-osd.rules /lib/udev/rules.d/

sudo apt-get install python2.7
sudo cp -r ~/zhao/site-packages/* /usr/lib/python2.7/dist-packages/

sudo vi /etc/environment
PYTHONHOME=/usr
PYTHONPATH=/usr/lib/python2.7
PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:$PYTHONHOME:$PYTHONPATH"

sudo -i
vi .bashrc
export PYTHONHOME=/usr
export PYTHONPATH=/usr/lib/python2.7
export PATH=$PATH:$PYTHONHOME:$PYTHONPATH

source /etc/environment
cd ~/zhao
sudo python get-pip.py

cd /usr/local/lib/python2.7
sudo -s
easy_install ceph_detect_init-1.0.1-py2.7.egg
easy_install ceph_disk-1.0.0-py2.7.egg

ln -s /bin/ceph-rbdnamer /usr/bin/ceph-rbdnamer
ln -s /usr/local/bin/ceph-disk /usr/bin/ceph-disk

sudo stop ceph-osd-all
sudo umount /dev/sda1
sudo umount /dev/sdb1

sudo vi /etc/fstab
删除硬盘挂载
sudo mount /dev/mmcblk0p1 /tmp
sudo cp ~/zhao/zImage /tmp/
sudo reboot

sudo vi /etc/ntp.conf
server 192.168.99.80

ceph-deploy --cluster xfsclustera admin mnc52
ceph-deploy --cluster xfsclustera disk zap mnc52:sda
ceph-deploy --cluster xfsclustera osd prepare mnc52:sda
ceph-deploy --cluster xfsclustera osd activate mnc52:/dev/sda1

ceph-deploy --cluster xfsclustera disk zap mnc52:sdb
ceph-deploy --cluster xfsclustera osd prepare mnc52:sdb
ceph-deploy --cluster xfsclustera osd activate mnc52:/dev/sdb1

osd_pool_default_size = 2

filestore_xattr_use_omap = true
osd_max_object_name_len = 256
osd_max_object_namespace_len = 64

erasure_code_dir = /lib/ceph/erasure-code
compression_dir = /lib/ceph/compressor
osd_class_dir = /lib/rados-classes
plugin_dir = /lib/ceph

public_network = 192.168.99.0/24


python ucs-2与ucs-4编码方式判断
以 --enable-unicode=ucs4 编译则：
>>> import sys
>>> print sys.maxunicode
1114111

以 --enable-unicode=ucs2 编译则：
>>> import sys
>>> print sys.maxunicode
65535

tab 有时出现下面的错误
-bash: cannot create temp file for here-document: Permission denied
sudo chmod 1777 /tmp

./ceph -s
./ceph osd tree

./ceph osd pool create data 128
./ceph osd pool create metadata 128
./ceph fs new arm_fs metadata data

sudo rm /etc/ceph/*
sudo rm -r /var/lib/ceph/mon/*
sudo rm -r /var/lib/ceph/osd/*
sudo rm -r /var/lib/ceph/mds/*
sudo rm -r /var/lib/ceph/tmp/*
sudo rm /var/lib/ceph/bootstrap-mds/*
sudo rm /var/lib/ceph/bootstrap-osd/*
sudo rm /var/lib/ceph/bootstrap-rgw/*
sudo rm /var/log/ceph/*

我们能做的事情：
了解Ceph的整体架构以及存储原理
掌握针对arm平台Ceph源代码编译(Cross compile, Cmake, Makefile)
熟悉根据log定位和解决Ceph部署及运维中的各种常见问题的解决(/var/log/ceph, /var/log/upstart, dmesg)
了解Ceph测试及性能调优(hardware selection, fio, iperf, pg distribution, crush map, pg num setting)
arm平台下Ceph文件存储和块存储的应用(3 Mon, 50 OSD, 360TB)


我们现在还没有做的事情：
ceph管控平台calamari的部署与维护
Ansible等自动化运维工具的应用
ceph做为openstack后端存储的应用


linux下代码行统计工具
find ./ -regextype posix-extended -regex ".*\.(h|c|cc)" |xargs wc -l
find . -name *.h |xargs wc -l
find . -name *.cc |xargs wc -l

rbd --cluster xfsclustera map rbd/foo --id admin --keyfile ./xfsclustera.client.admin.keyring
sudo rbd map test/image --name client.admin  -m 192.168.99.21,192.168.99.22,192.168.99.23 -k /etc/ceph/client.admin.keyring
sudo rbd --cluster xfsclustera map rbd/foo --name client.admin  -m 192.168.99.51 -k ./xfsclustera.client.admin.keyring

ubuntu 32位兼容包
sudo apt-get install lib32z1

1.在kernel.org 下载4.4.52的kernel source
2.

export CROSS_COMPILE=/home/mac/projects/armv7-marvell-linux-gnueabihf-hard-5.2.1_i686_20151110/bin/
export ARCH=arm
make mvebu_v7_lsp_defconfig
make mvebu_v7_defconfig
make 
make zImage
make INSTALL_MOD_PATH=/home/mac/projects/rootfs_for_build_kernel modules_install
make INSTALL_MOD_PATH=/home/mac/projects/rootfs_for_build_kernel_nolsp modules_install
make INSTALL_MOD_PATH=/home/mac/projects/rootfs_for_build_kernel_nolsp_xfs modules_install
_

vi ./include/config/kernel.release
4.4.52-devel-17.04.2
4.4.52-devel-17.04.2-01567-g83d3d84
XFS filesystem support (XFS_FS) [M/n/y/?] m
  XFS Quota support (XFS_QUOTA) [N/y/?] (NEW) y
  XFS POSIX ACL support (XFS_POSIX_ACL) [N/y/?] (NEW) y
  XFS Realtime subvolume support (XFS_RT) [N/y/?] (NEW) y
  XFS Verbose Warnings (XFS_WARN) [N/y/?] (NEW) y
  XFS Debugging support (XFS_DEBUG) [N/y/?] (NEW) y

先保存你的.config, 然后再make distclean一下试试

修改已有deb包
自己创建deb所需目录结构(控制信息和安装内容)，然后打包，一般使用这种方法来修改已有的deb包，而不是新建deb包，命令如下：
$ dpkg -X xxx.deb test // 解包安装内容
$ cd test
$ dpkg -e ../xxx.deb // 解包控制信息
修改其中内容
$ cd ../
$ dpkg -b dirname xxx_new.deb // 重新打包

